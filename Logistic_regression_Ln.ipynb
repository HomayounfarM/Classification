{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNZIjEkOqf8Nir+9vhsb3A8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HomayounfarM/Classification/blob/main/Logistic_regression_Ln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a statistical method used for modeling a binary dependent variable, meaning the outcome has two possible values (e.g., success/failure, yes/no, high/low). It is widely used in various fields such as medicine, social sciences, and machine learning for classification tasks.\n",
        "\n",
        "To illustrate how the logistic regression works behind the scenes, I walk you through an example defining all required functions. We also do the same using the ski-learning package.\n",
        "Assume we study the effect of the phosphorous and its interaction with the soil pH on yield in a bean experiment. we have a dataset including\n",
        " the M3-P soil extractant (continuous variable), soil pH (continuous variable) and the yield (categorized as 'high' or 'low'). For the yield, 1 means ‘high’ and 0 means ‘low’.\n"
      ],
      "metadata": {
        "id": "hhnvEhW1Ft8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "import io\n",
        "import ssl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-BWNAdFmtb8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset from GITHUB\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "url = \"https://raw.githubusercontent.com/HomayounfarM/Classification/main/data/ex2data2.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.columns = ['pH', 'M3_P', 'Yield']\n",
        "df['pH_r'] = (df['pH']-np.mean(df['pH']))/np.std(df['pH'])\n",
        "df['M3_P_r'] = (df['M3_P']-np.mean(df['M3_P']))/np.std(df['M3_P'])\n",
        "\n",
        "\n",
        "df.head(5)\n"
      ],
      "metadata": {
        "id": "6nSMrC1wqYIc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCd3ThgMj3aP"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "X_scaled = np.array(df[['pH_r', 'M3_P_r']])\n",
        "YY = np.array(df['Yield'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, YY, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df['pH_r']\n",
        "x2 = df['M3_P_r']\n",
        "y = df['Yield']\n",
        "\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "8tnZA6sFt_W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sigmoid function"
      ],
      "metadata": {
        "id": "B3gCJNLSuEl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "\n",
        "    g = 1/(1+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "l5zkM2-xuI_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"sigmoid(0) = \" + str(sigmoid(0)))"
      ],
      "metadata": {
        "id": "Wjldgph4uN11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost function\n",
        "def compute_cost(X, y, w, b):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    z = X.dot(w)+b\n",
        "    f = sigmoid(z)\n",
        "    total_cost = (1/m) * (-y.T.dot(np.log(f))- (1-y).T.dot(np.log(1-f)))\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "SnTS9qXDuTii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = X_train.shape\n",
        "\n",
        "# Compute and display cost with w initialized to zeroes\n",
        "initial_w = np.zeros(n)\n",
        "initial_b = 0.\n",
        "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
        "print('Cost at initial w (zeros): {:.3f}'.format(cost))"
      ],
      "metadata": {
        "id": "OikCHlaUuYHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent\n",
        "def compute_gradient(X, y, w, b):\n",
        "\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0.\n",
        "\n",
        "    y = y.reshape(-1,1)\n",
        "    w = w.reshape(-1,1)\n",
        "\n",
        "    f = sigmoid(X.dot(w)+b)\n",
        "    diff_fy = f-y\n",
        "    dj_dww = (1/m) * diff_fy.T.dot(X)\n",
        "    dj_dww = dj_dww.T\n",
        "    dj_dw = np.reshape(dj_dww, -1)\n",
        "    dj_db = (1/m) * (dj_db + np.sum(diff_fy))\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "kYmBTfbUudcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display gradient with w initialized to zeroes\n",
        "initial_w = np.zeros(n)\n",
        "initial_b = 0.\n",
        "\n",
        "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
        "print(f'dj_db at initial w (zeros):{dj_db}' )\n",
        "print(f'dj_dw at initial w (zeros):{dj_dw.tolist()}' )"
      ],
      "metadata": {
        "id": "dLNf4-ZZuizK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display cost and gradient with non-zero w\n",
        "test_w = np.array([ 0.2, -0.5])\n",
        "test_b = -24\n",
        "dj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)\n",
        "\n",
        "print('dj_db at test_w:', dj_db)\n",
        "print('dj_dw at test_w:', dj_dw.tolist())\n"
      ],
      "metadata": {
        "id": "2x7mnBjWupEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performs batch gradient descent to learn theta. Updates theta by taking num_iters gradient steps with learning rate alpha\n",
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "\n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in)\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w_in = w_in - alpha * dj_dw\n",
        "        b_in = b_in - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            cost =  cost_function(X, y, w_in, b_in)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
        "            w_history.append(w_in)\n",
        "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
        "\n",
        "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "ylVzpIoluuUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "intial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)\n",
        "initial_b = -8\n",
        "\n",
        "\n",
        "# Some gradient descent settings\n",
        "iterations = 100000\n",
        "alpha = 0.7\n",
        "\n",
        "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b,\n",
        "                                   compute_cost, compute_gradient, alpha, iterations)\n",
        "intial_w, initial_b, w,b"
      ],
      "metadata": {
        "id": "s6JfAixsuzIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_temp = np.linspace(min(x1), max(x1), num=len(x1))\n",
        "y_predicted = -(w[1]*x_temp+b)/w[0]\n",
        "\n",
        "plt.plot(x_temp, y_predicted, color='purple')\n",
        "\n",
        "-w[1]/w[0], -b/w[0], -intial_w[1]/intial_w[0], -initial_b/intial_w[0], intial_w[0],intial_w[1],initial_b"
      ],
      "metadata": {
        "id": "HindaqN0u3yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting the decision boundary\n",
        "\n",
        "Try to plot decision boundary from got in previous step."
      ],
      "metadata": {
        "id": "MRhAKU4lvB-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df['pH_r']\n",
        "x2 = df['M3_P_r']\n",
        "y = df['Yield']\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "#add line to show fitted polynomial regression model\n",
        "plt.plot(x_temp, y_predicted, color='purple')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "RmQFKKhKvFIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction function\n",
        "def predict(X, w, b):\n",
        "\n",
        "    # number of training examples\n",
        "    m, n = X.shape\n",
        "    p = np.zeros(m)\n",
        "\n",
        "    z = X.dot(w)+b\n",
        "    f = sigmoid(z)\n",
        "    p = f >= 0.5\n",
        "\n",
        "    return p"
      ],
      "metadata": {
        "id": "-LgvQXXOvOKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your predict code\n",
        "np.random.seed(1)\n",
        "tmp_w = np.random.randn(2)\n",
        "tmp_b = 0.3\n",
        "tmp_X = np.random.randn(50, 2) - 0.5\n",
        "\n",
        "tmp_p = predict(tmp_X, tmp_w, tmp_b)\n",
        "print(f'Output of predict: shape {tmp_p.shape}, value {tmp_p}')"
      ],
      "metadata": {
        "id": "okWZlqkevRal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute accuracy on our training set\n",
        "p = predict(X_train, w,b)\n",
        "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
      ],
      "metadata": {
        "id": "vYtz6XuqvUFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using skit-learn"
      ],
      "metadata": {
        "id": "vHdM3OxNJG0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "XX = np.array(df[['pH', 'M3_P']])\n",
        "YY = np.array(df['Yield'])\n",
        "\n",
        "#Feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scale=StandardScaler()\n",
        "X_scaled = scale.fit_transform(XX)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, YY, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Extract the coefficients\n",
        "coefficients = model.coef_[0]\n",
        "intercept = model.intercept_[0]\n",
        "coefficients, intercept\n",
        "\n",
        "#Plotting the decision boundary\n",
        "x1 = X_scaled[:,0]\n",
        "x2 = X_scaled[:,1]\n",
        "y = YY\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "#add line to show fitted polynomial regression model\n",
        "x_temp = np.linspace(min(x1), max(x1), num=len(x1))\n",
        "y_predicted = -(coefficients[1]*x_temp+intercept)/coefficients[0]\n",
        "\n",
        "plt.plot(x_temp, y_predicted, color='purple')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "0RUZ7s77Jwjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization:\n",
        "\n",
        "Regularization is a technique used to prevent overfitting.\n",
        "Indeed, there are different ways to handle overfitting as follows:\n",
        "\n",
        "1- collecting more data: sometimes, this option is not working.\n",
        "\n",
        "2- Feature selection. Selecting the most effective features.\n",
        "\n",
        "3- Reduce the size of the parameters or regularize the parameters.\n",
        "\n",
        "Here, I explain how regularization works in a logistic regression.\n",
        "\n",
        "Regularization is typically adding a penalty term to the loss function, which penalizes the coefficients of the model.\n",
        "\n",
        "There are two common types of regularization:\n",
        "\n",
        "1- L1 Regularization (Lasso Regularization):\n",
        "\n",
        "It  adds a penalty proportional to the absolute value of the coefficients leading some coefficients to be exactly zero, effectively performing feature selection.\n",
        "\n",
        "2- L2 Regularization (Ridge Regularization):\n",
        "\n",
        "It adds a penalty proportional to the square of the coefficients leading to small but non-zero coefficients."
      ],
      "metadata": {
        "id": "Gh-Ctp2dcC8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset from GITHUB\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "url = \"https://raw.githubusercontent.com/HomayounfarM/Classification/main/data/ex2data2.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.columns = ['pH', 'M3_P', 'Yield']\n",
        "df['pH_r'] = (df['pH']-np.mean(df['pH']))/np.std(df['pH'])\n",
        "df['M3_P_r'] = (df['M3_P']-np.mean(df['M3_P']))/np.std(df['M3_P'])\n",
        "\n",
        "\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "6_wMJi7c1O_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "X_scaled = np.array(df[['pH_r', 'M3_P_r']])\n",
        "YY = np.array(df['Yield'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, YY, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xkr3AYtp12DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df['pH_r']\n",
        "x2 = df['M3_P_r']\n",
        "y = df['Yield']\n",
        "\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "yp9qve8u32f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_feature(X, Y):\n",
        "  # demonstrate the types of features created\n",
        "  import numpy as np\n",
        "  from sklearn.preprocessing import PolynomialFeatures\n",
        "  # define the dataset\n",
        "  data = np.vstack([X,Y]).T\n",
        "  # perform a polynomial features transform of the dataset\n",
        "  poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "  data = poly.fit_transform(data)\n",
        "  return data"
      ],
      "metadata": {
        "id": "YnPQUtPq17D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original shape of data:\", X_train.shape)\n",
        "\n",
        "mapped_X =  map_feature(X_train[:, 0], X_train[:, 1])\n",
        "print(\"Shape after feature mapping:\", mapped_X.shape)"
      ],
      "metadata": {
        "id": "rMxQ2_eX19zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train[0]:\", X_train[0])\n",
        "print(\"mapped X_train[0]:\", mapped_X[0])"
      ],
      "metadata": {
        "id": "qLIJitG82AuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "\n",
        "    g = 1/(1+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "5yK19Ko-6uxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost function\n",
        "def compute_cost(X, y, w, b, lambda_= 1):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    z = X.dot(w)+b\n",
        "    f = sigmoid(z)\n",
        "    total_cost = (1/m) * (-y.T.dot(np.log(f))- (1-y).T.dot(np.log(1-f)))\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "Y_heJsnR61S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent\n",
        "def compute_gradient(X, y, w, b, lambda_=None):\n",
        "\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0.\n",
        "\n",
        "    y = y.reshape(-1,1)\n",
        "    w = w.reshape(-1,1)\n",
        "\n",
        "    f = sigmoid(X.dot(w)+b)\n",
        "    diff_fy = f-y\n",
        "    dj_dww = (1/m) * diff_fy.T.dot(X)\n",
        "    dj_dww = dj_dww.T\n",
        "    dj_dw = np.reshape(dj_dww, -1)\n",
        "    dj_db = (1/m) * (dj_db + np.sum(diff_fy))\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "_krxyN8D8WJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X :    (array_like Shape (m, n)\n",
        "      y :    (array_like Shape (m,))\n",
        "      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
        "      b_in : (scalar)                 Initial value of parameter of the model\n",
        "      cost_function:                  function to compute cost\n",
        "      alpha : (float)                 Learning rate\n",
        "      num_iters : (int)               number of iterations to run gradient descent\n",
        "      lambda_ (scalar, float)         regularization constant\n",
        "\n",
        "    Returns:\n",
        "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
        "          running gradient descent\n",
        "      b : (scalar)                Updated value of parameter of the model after\n",
        "          running gradient descent\n",
        "    \"\"\"\n",
        "\n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w_in = w_in - alpha * dj_dw\n",
        "        b_in = b_in - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
        "            w_history.append(w_in)\n",
        "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
        "\n",
        "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "wuRvqnGS7UfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "      X : (array_like Shape (m,n)) data, m examples by n features\n",
        "      y : (array_like Shape (m,)) target value\n",
        "      w : (array_like Shape (n,)) Values of parameters of the model\n",
        "      b : (array_like Shape (n,)) Values of bias parameter of the model\n",
        "      lambda_ : (scalar, float)    Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost: (scalar)         cost\n",
        "    \"\"\"\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    # Calls the compute_cost function that you implemented above\n",
        "    cost_without_reg = compute_cost(X, y, w, b)\n",
        "\n",
        "    # You need to calculate this value\n",
        "    reg_cost = 0.\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    reg_cost = np.sum(w**2)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Add the regularization cost to get the total cost\n",
        "    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "9g1eiY8x2DeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n))   variable such as house size\n",
        "      y : (ndarray Shape (m,))    actual value\n",
        "      w : (ndarray Shape (n,))    values of parameters of the model\n",
        "      b : (scalar)                value of parameter of the model\n",
        "      lambda_ : (scalar,float)    regularization constant\n",
        "    Returns\n",
        "      dj_db: (scalar)             The gradient of the cost w.r.t. the parameter b.\n",
        "      dj_dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.\n",
        "\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "\n",
        "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    dj_dw = dj_dw + (lambda_/m) * w\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "I0zX-AJgT4o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display cost with w initialized to zeroes\n",
        "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
        "\n",
        "np.random.seed(1)\n",
        "initial_w = 0.01 * (np.random.rand(X_mapped.shape[1]) - 0.5)\n",
        "initial_b = -8\n",
        "\n",
        "lambda_ = 0.5\n",
        "\n",
        "cost = compute_cost_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
        "print(\"Regularized cost :\", cost)"
      ],
      "metadata": {
        "id": "X96-f-wFE1i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
        "\n",
        "np.random.seed(1)\n",
        "initial_w = 0.01 * (np.random.rand(X_mapped.shape[1]) - 0.5)\n",
        "initial_b = -8\n",
        "\n",
        "lambda_ = 0.05\n",
        "\n",
        "# Some gradient descent settings\n",
        "iterations = 100000\n",
        "alpha = 0.01\n",
        "\n",
        "w,b, J_history,_ = gradient_descent(X_mapped, y_train, initial_w, initial_b,\n",
        "                                    compute_cost_reg, compute_gradient_reg,\n",
        "                                    alpha, iterations, lambda_)\n",
        "initial_w, initial_b, w,b\n"
      ],
      "metadata": {
        "id": "hbIbkTs02GM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting the decision boundary\n",
        "\n",
        "Try to plot decision boundary from got in previous step."
      ],
      "metadata": {
        "id": "Yf41SEWEUsmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute accuracy on the training set\n",
        "p = predict(X_mapped, w, b)\n",
        "\n",
        "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
      ],
      "metadata": {
        "id": "eco4KBOR2Q5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_temp = np.linspace(min(x1), max(x1), num=len(x1))\n",
        "y_predicted = -(w[1]*x_temp+b)/w[0]\n",
        "\n",
        "\n",
        "part1 = -(w[1]+w[4]*x_temp)/(2*w[3])\n",
        "discriminant = (w[1]+w[4]*x_temp)**2-4*w[3]*(x_temp*w[0]+x_temp**2*w[2]+b)\n",
        "\n",
        "# Check if there are real roots\n",
        "if discriminant.any() >= 0:\n",
        "    # Calculate the roots using the quadratic formula\n",
        "    root1 = (part1 + np.sqrt(discriminant)) / (2 * w[2])\n",
        "    root2 = (part1 - np.sqrt(discriminant)) / (2 * w[2])\n",
        "\n"
      ],
      "metadata": {
        "id": "yX7dQn4y2TUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "df_temp = pd.DataFrame(w*X_mapped+b)\n",
        "df_temp.head(5)\n",
        "y_temp = np.zeros(len(df_temp))\n",
        "\n",
        "# Fit linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(df_temp, y_temp)\n",
        "\n",
        "x_temp = np.linspace(min(x1), max(x1), num=len(x1))\n",
        "y_pred = model.predict(x_temp)"
      ],
      "metadata": {
        "id": "JxHyCnRweayT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df['pH_r']\n",
        "x2 = df['M3_P_r']\n",
        "y = df['Yield']\n",
        "\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "#add line to show fitted polynomial regression model\n",
        "plt.plot(x_temp,root2, color='purple')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "MKjXNM5k2Vf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Skit-Learn\n",
        "\n"
      ],
      "metadata": {
        "id": "GBWpqn5ZoJz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_feature(X, Y):\n",
        "  # demonstrate the types of features created\n",
        "  import numpy as np\n",
        "  from sklearn.preprocessing import PolynomialFeatures\n",
        "  # define the dataset\n",
        "  data = np.vstack([X,Y]).T\n",
        "  # perform a polynomial features transform of the dataset\n",
        "  poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "  data = poly.fit_transform(data)\n",
        "  return data"
      ],
      "metadata": {
        "id": "xglfsSwSpFl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "XX = np.array(df[['pH', 'M3_P']])\n",
        "YY = np.array(df['Yield'])\n",
        "\n",
        "#Feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scale=StandardScaler()\n",
        "X_scaled = scale.fit_transform(XX)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, YY, test_size=0.2, random_state=42)\n",
        "\n",
        "X_mapped_train = map_feature(X_train[:, 0], X_train[:, 1])\n",
        "X_mapped_test = map_feature(X_test[:, 0], X_test[:, 1])\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "# C parameter controls the regularization strength (inverse of regularization strength)\n",
        "# Higher C means less regularization (default C=1.0)\n",
        "model = LogisticRegression(penalty='l2', C=15, solver='lbfgs', max_iter=10000, random_state=1)\n",
        "\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_mapped_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_mapped_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Extract the coefficients\n",
        "coefficients = model.coef_[0]\n",
        "intercept = model.intercept_[0]\n",
        "coefficients, intercept\n",
        "\n",
        "#Plotting the decision boundary\n",
        "x1 = X_scaled[:,0]\n",
        "x2 = X_scaled[:,1]\n",
        "y = YY\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "#add line to show fitted polynomial regression model\n",
        "x_temp = np.linspace(min(x1), max(x1), num=len(x1))\n",
        "y_predicted = -(coefficients[1]*x_temp+intercept)/coefficients[0]\n",
        "\n",
        "# Deriving the relationship between x1 and x2\n",
        "part1 = -(coefficients[1]+coefficients[4]*x_temp)/(2*coefficients[3])\n",
        "discriminant = (coefficients[1]+coefficients[4]*x_temp)**2-4*coefficients[3]*(x_temp*coefficients[0]+x_temp**2*coefficients[2]+intercept)\n",
        "# Check if there are real roots\n",
        "if discriminant.any() >= 0:\n",
        "    # Calculate the roots using the quadratic formula\n",
        "    root1 = (part1 + np.sqrt(discriminant)) / (2 * w[2])\n",
        "    root2 = (part1 - np.sqrt(discriminant)) / (2 * w[2])\n",
        "\n",
        "plt.plot(x_temp, root2, color='purple')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])\n",
        "coefficients"
      ],
      "metadata": {
        "id": "SNUet9SGoIxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def PolyCoefficients(x, coeffs):\n",
        "    \"\"\" Returns a polynomial for ``x`` values for the ``coeffs`` provided.\n",
        "\n",
        "    The coefficients must be in ascending order (``x**0`` to ``x**o``).\n",
        "    \"\"\"\n",
        "    o = len(coeffs)\n",
        "    print(f'# This is a polynomial of order {o}.')\n",
        "    y = 0\n",
        "    for i in range(o):\n",
        "        y += coeffs[i]*x**i\n",
        "    return y\n",
        "\n",
        "x = np.linspace(0, 9, 10)\n",
        "coeffs = [1, 2, 3, 4, 5]\n",
        "plt.plot(x, PolyCoefficients(x, coeffs))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fqr-NwZF2Ydw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTPNplPQd4K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "hU5eVpYe1ZXU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}