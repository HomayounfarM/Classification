{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPYx4t7J1Ap9pWygoB6BxZ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HomayounfarM/Classification/blob/main/Logistic_regression_Ln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a statistical method used for modeling a binary dependent variable, meaning the outcome has two possible values (e.g., success/failure, yes/no, high/low). It is widely used in various fields such as medicine, social sciences, and machine learning for classification tasks.\n",
        "\n",
        "To illustrate how the logistic regression works behind the scenes, I walk you through an example defining all required functions. We also do the same using the ski-learning package.\n",
        "Assume we study the effect of the phosphorous and its interaction with the soil pH on yield in a bean experiment. we have a dataset including\n",
        " the M3-P soil extractant (continuous variable), soil pH (continuous variable) and the yield (categorized as 'high' or 'low'). For the yield, 1 means ‘high’ and 0 means ‘low’.\n"
      ],
      "metadata": {
        "id": "hhnvEhW1Ft8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "import io\n",
        "import ssl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-BWNAdFmtb8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset from GITHUB\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "url = \"https://raw.githubusercontent.com/HomayounfarM/Classification/main/data/ex2data2.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.columns = ['pH', 'M3_P', 'Yield']\n",
        "df['pH_r'] = (df['pH']-np.mean(df['pH']))/np.std(df['pH'])\n",
        "df['M3_P_r'] = (df['M3_P']-np.mean(df['M3_P']))/np.std(df['M3_P'])\n",
        "\n",
        "\n",
        "df.head(5)\n"
      ],
      "metadata": {
        "id": "6nSMrC1wqYIc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCd3ThgMj3aP"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "X_scaled = np.array(df[['pH_r', 'M3_P_r']])\n",
        "YY = np.array(df['Yield'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, YY, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df['pH_r']\n",
        "x2 = df['M3_P_r']\n",
        "y = df['Yield']\n",
        "\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "8tnZA6sFt_W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sigmoid function"
      ],
      "metadata": {
        "id": "B3gCJNLSuEl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "\n",
        "    g = 1/(1+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "l5zkM2-xuI_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"sigmoid(0) = \" + str(sigmoid(0)))"
      ],
      "metadata": {
        "id": "Wjldgph4uN11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost function\n",
        "def compute_cost(X, y, w, b):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    z = X.dot(w)+b\n",
        "    f = sigmoid(z)\n",
        "    total_cost = (1/m) * (-y.T.dot(np.log(f))- (1-y).T.dot(np.log(1-f)))\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "SnTS9qXDuTii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = X_train.shape\n",
        "\n",
        "# Compute and display cost with w initialized to zeroes\n",
        "initial_w = np.zeros(n)\n",
        "initial_b = 0.\n",
        "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
        "print('Cost at initial w (zeros): {:.3f}'.format(cost))"
      ],
      "metadata": {
        "id": "OikCHlaUuYHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent\n",
        "def compute_gradient(X, y, w, b):\n",
        "\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0.\n",
        "\n",
        "    y = y.reshape(-1,1)\n",
        "    w = w.reshape(-1,1)\n",
        "\n",
        "    f = sigmoid(X.dot(w)+b)\n",
        "    diff_fy = f-y\n",
        "    dj_dww = (1/m) * diff_fy.T.dot(X)\n",
        "    dj_dww = dj_dww.T\n",
        "    dj_dw = np.reshape(dj_dww, -1)\n",
        "    dj_db = (1/m) * (dj_db + np.sum(diff_fy))\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "kYmBTfbUudcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display gradient with w initialized to zeroes\n",
        "initial_w = np.zeros(n)\n",
        "initial_b = 0.\n",
        "\n",
        "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
        "print(f'dj_db at initial w (zeros):{dj_db}' )\n",
        "print(f'dj_dw at initial w (zeros):{dj_dw.tolist()}' )"
      ],
      "metadata": {
        "id": "dLNf4-ZZuizK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display cost and gradient with non-zero w\n",
        "test_w = np.array([ 0.2, -0.5])\n",
        "test_b = -24\n",
        "dj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)\n",
        "\n",
        "print('dj_db at test_w:', dj_db)\n",
        "print('dj_dw at test_w:', dj_dw.tolist())\n"
      ],
      "metadata": {
        "id": "2x7mnBjWupEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performs batch gradient descent to learn theta. Updates theta by taking num_iters gradient steps with learning rate alpha\n",
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "\n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in)\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w_in = w_in - alpha * dj_dw\n",
        "        b_in = b_in - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            cost =  cost_function(X, y, w_in, b_in)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
        "            w_history.append(w_in)\n",
        "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
        "\n",
        "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "ylVzpIoluuUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "intial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)\n",
        "initial_b = -8\n",
        "\n",
        "\n",
        "# Some gradient descent settings\n",
        "iterations = 100000\n",
        "alpha = 0.7\n",
        "\n",
        "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b,\n",
        "                                   compute_cost, compute_gradient, alpha, iterations)\n",
        "intial_w, initial_b, w,b"
      ],
      "metadata": {
        "id": "s6JfAixsuzIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_temp = np.linspace(min(x1), max(x1), num=len(x1))\n",
        "y_predicted = -(w[1]*x_temp+b)/w[0]\n",
        "\n",
        "plt.plot(x_temp, y_predicted, color='purple')\n",
        "\n",
        "-w[1]/w[0], -b/w[0], -intial_w[1]/intial_w[0], -initial_b/intial_w[0], intial_w[0],intial_w[1],initial_b"
      ],
      "metadata": {
        "id": "HindaqN0u3yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting the decision boundary\n",
        "\n",
        "Try to plot decision boundary from got in previous step."
      ],
      "metadata": {
        "id": "MRhAKU4lvB-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df['pH_r']\n",
        "x2 = df['M3_P_r']\n",
        "y = df['Yield']\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "#add line to show fitted polynomial regression model\n",
        "plt.plot(x_temp, y_predicted, color='purple')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "RmQFKKhKvFIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction function\n",
        "def predict(X, w, b):\n",
        "\n",
        "    # number of training examples\n",
        "    m, n = X.shape\n",
        "    p = np.zeros(m)\n",
        "\n",
        "    z = X.dot(w)+b\n",
        "    f = sigmoid(z)\n",
        "    p = f >= 0.5\n",
        "\n",
        "    return p"
      ],
      "metadata": {
        "id": "-LgvQXXOvOKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your predict code\n",
        "np.random.seed(1)\n",
        "tmp_w = np.random.randn(2)\n",
        "tmp_b = 0.3\n",
        "tmp_X = np.random.randn(50, 2) - 0.5\n",
        "\n",
        "tmp_p = predict(tmp_X, tmp_w, tmp_b)\n",
        "print(f'Output of predict: shape {tmp_p.shape}, value {tmp_p}')"
      ],
      "metadata": {
        "id": "okWZlqkevRal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute accuracy on our training set\n",
        "p = predict(X_train, w,b)\n",
        "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
      ],
      "metadata": {
        "id": "vYtz6XuqvUFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using skit-learn"
      ],
      "metadata": {
        "id": "vHdM3OxNJG0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "XX = np.array(df[['pH', 'M3_P']])\n",
        "YY = np.array(df['Yield'])\n",
        "\n",
        "#Feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scale=StandardScaler()\n",
        "X_scaled = scale.fit_transform(XX)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, YY, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Extract the coefficients\n",
        "coefficients = model.coef_[0]\n",
        "intercept = model.intercept_[0]\n",
        "coefficients, intercept\n",
        "\n",
        "#Plotting the decision boundary\n",
        "x1 = X_scaled[:,0]\n",
        "x2 = X_scaled[:,1]\n",
        "y = YY\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "#add line to show fitted polynomial regression model\n",
        "x_temp = np.linspace(min(x1), max(x1), num=len(x1))\n",
        "y_predicted = -(coefficients[1]*x_temp+intercept)/coefficients[0]\n",
        "\n",
        "plt.plot(x_temp, y_predicted, color='purple')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "0RUZ7s77Jwjm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}