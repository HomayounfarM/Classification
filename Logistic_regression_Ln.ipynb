{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO60N7kw2V9GDX9bFiIDDAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HomayounfarM/Classification/blob/main/Logistic_regression_Ln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a statistical method used for modeling a binary dependent variable, meaning the outcome has two possible values (e.g., success/failure, yes/no, high/low). It is widely used in various fields such as medicine, social sciences, and machine learning for classification tasks.\n",
        "\n",
        "To illustrate how the logistic regression works behind the scenes, I walk you through an example defining all required functions. We also do the same using the ski-learning package.\n",
        "Assume we study the effect of the phosphorous and its interaction with the soil pH on yield in a bean experiment. we have a dataset including\n",
        " the M3-P soil extractant (continuous variable), soil pH (continuous variable) and the yield (categorized as 'high' or 'low'). For the yield, 1 means ‘high’ and 0 means ‘low’.\n"
      ],
      "metadata": {
        "id": "hhnvEhW1Ft8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "import io\n",
        "import ssl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-BWNAdFmtb8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset from GITHUB\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "url = \"https://raw.githubusercontent.com/HomayounfarM/Classification/main/data/ex2data2.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.columns = ['pH', 'M3_P', 'Yield']\n",
        "df['pH_r'] = (df['pH']-np.mean(df['pH']))/np.std(df['pH'])\n",
        "df['M3_P_r'] = (df['M3_P']-np.mean(df['M3_P']))/np.std(df['M3_P'])\n",
        "\n",
        "\n",
        "df.head(5)\n"
      ],
      "metadata": {
        "id": "6nSMrC1wqYIc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCd3ThgMj3aP"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "X_scaled = np.array(df[['pH_r', 'M3_P_r']])\n",
        "YY = np.array(df['Yield'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, YY, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df['pH_r']\n",
        "x2 = df['M3_P_r']\n",
        "y = df['Yield']\n",
        "\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "8tnZA6sFt_W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sigmoid function"
      ],
      "metadata": {
        "id": "B3gCJNLSuEl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "\n",
        "    g = 1/(1+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "l5zkM2-xuI_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"sigmoid(0) = \" + str(sigmoid(0)))"
      ],
      "metadata": {
        "id": "Wjldgph4uN11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "\n",
        "    # number of training examples\n",
        "    m, n = X.shape\n",
        "    p = np.zeros(m)\n",
        "\n",
        "    z = X.dot(w)+b\n",
        "    p = sigmoid(z)\n",
        "    f = p >= 0.5\n",
        "\n",
        "    return p, f"
      ],
      "metadata": {
        "id": "6eCcfgduhsui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = X_train.shape\n",
        "initial_w = np.zeros(n)\n",
        "initial_b = 0.\n",
        "print (\"predict(X_train, initial_w, initial_b) = \" + str(predict(X_train, initial_w, initial_b)[0]))"
      ],
      "metadata": {
        "id": "VGFpRXYnh9is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost function\n",
        "def compute_cost(X, y, w, b):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    z = X.dot(w)+b\n",
        "    f = sigmoid(z)\n",
        "    total_cost = (1/m) * (-y.T.dot(np.log(f))- (1-y).T.dot(np.log(1-f)))\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "SnTS9qXDuTii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = X_train.shape\n",
        "\n",
        "# Compute and display cost with w initialized to zeroes\n",
        "initial_w = np.zeros(n)\n",
        "initial_b = 0.\n",
        "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
        "print('Cost at initial w (zeros): {:.3f}'.format(cost))"
      ],
      "metadata": {
        "id": "OikCHlaUuYHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent\n",
        "def compute_gradient(X, y, w, b):\n",
        "\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0.\n",
        "\n",
        "    y = y.reshape(-1,1)\n",
        "    w = w.reshape(-1,1)\n",
        "\n",
        "    f = sigmoid(X.dot(w)+b)\n",
        "    diff_fy = f-y\n",
        "    dj_dww = (1/m) * diff_fy.T.dot(X)\n",
        "    dj_dww = dj_dww.T\n",
        "    dj_dw = np.reshape(dj_dww, -1)\n",
        "    dj_db = (1/m) * (dj_db + np.sum(diff_fy))\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "kYmBTfbUudcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display gradient with w initialized to zeroes\n",
        "initial_w = np.zeros(n)\n",
        "initial_b = 0.\n",
        "\n",
        "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
        "print(f'dj_db at initial w (zeros):{dj_db}' )\n",
        "print(f'dj_dw at initial w (zeros):{dj_dw.tolist()}' )"
      ],
      "metadata": {
        "id": "dLNf4-ZZuizK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display cost and gradient with non-zero w\n",
        "test_w = np.array([ 0.2, -0.5])\n",
        "test_b = -24\n",
        "dj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)\n",
        "\n",
        "print('dj_db at test_w:', dj_db)\n",
        "print('dj_dw at test_w:', dj_dw.tolist())\n"
      ],
      "metadata": {
        "id": "2x7mnBjWupEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performs batch gradient descent to learn theta. Updates theta by taking num_iters gradient steps with learning rate alpha\n",
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "\n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in)\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w_in = w_in - alpha * dj_dw\n",
        "        b_in = b_in - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            cost =  cost_function(X, y, w_in, b_in)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
        "            w_history.append(w_in)\n",
        "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
        "\n",
        "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "ylVzpIoluuUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "intial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)\n",
        "initial_b = -8\n",
        "\n",
        "\n",
        "# Some gradient descent settings\n",
        "iterations = 100000\n",
        "alpha = 0.7\n",
        "\n",
        "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b,\n",
        "                                   compute_cost, compute_gradient, alpha, iterations)\n",
        "intial_w, initial_b, w,b"
      ],
      "metadata": {
        "id": "s6JfAixsuzIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_temp = np.linspace(min(x1), max(x1), num=len(x1))\n",
        "y_predicted = -(w[1]*x_temp+b)/w[0]\n",
        "\n",
        "plt.plot(x_temp, y_predicted, color='purple')\n",
        "\n",
        "-w[1]/w[0], -b/w[0], -intial_w[1]/intial_w[0], -initial_b/intial_w[0], intial_w[0],intial_w[1],initial_b"
      ],
      "metadata": {
        "id": "HindaqN0u3yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting the decision boundary\n",
        "\n",
        "Try to plot decision boundary from got in previous step."
      ],
      "metadata": {
        "id": "MRhAKU4lvB-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df['pH_r']\n",
        "x2 = df['M3_P_r']\n",
        "y = df['Yield']\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "#add line to show fitted polynomial regression model\n",
        "plt.plot(x_temp, y_predicted, color='purple')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "RmQFKKhKvFIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your predict code\n",
        "np.random.seed(1)\n",
        "tmp_w = np.random.randn(2)\n",
        "tmp_b = 0.3\n",
        "tmp_X = np.random.randn(50, 2) - 0.5\n",
        "\n",
        "tmp_p = predict(tmp_X, tmp_w, tmp_b)[0]\n",
        "print(f'Output of predict: shape {tmp_p.shape}, value {tmp_p}')"
      ],
      "metadata": {
        "id": "okWZlqkevRal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute accuracy on our training set\n",
        "p = predict(X_train, w,b)\n",
        "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
      ],
      "metadata": {
        "id": "vYtz6XuqvUFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using skit-learn"
      ],
      "metadata": {
        "id": "vHdM3OxNJG0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "XX = np.array(df[['pH', 'M3_P']])\n",
        "YY = np.array(df['Yield'])\n",
        "\n",
        "#Feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scale=StandardScaler()\n",
        "X_scaled = scale.fit_transform(XX)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, YY, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Extract the coefficients\n",
        "coefficients = model.coef_[0]\n",
        "intercept = model.intercept_[0]\n",
        "coefficients, intercept\n",
        "\n",
        "#Plotting the decision boundary\n",
        "x1 = X_scaled[:,0]\n",
        "x2 = X_scaled[:,1]\n",
        "y = YY\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "#add line to show fitted polynomial regression model\n",
        "x_temp = np.linspace(min(x1), max(x1), num=len(x1))\n",
        "y_predicted = -(coefficients[1]*x_temp+intercept)/coefficients[0]\n",
        "\n",
        "plt.plot(x_temp, y_predicted, color='purple')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "0RUZ7s77Jwjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization:\n",
        "\n",
        "Regularization is a technique used to prevent overfitting.\n",
        "Indeed, there are different ways to handle overfitting as follows:\n",
        "\n",
        "1- collecting more data: sometimes, this option is not working.\n",
        "\n",
        "2- Feature selection. Selecting the most effective features.\n",
        "\n",
        "3- Reduce the size of the parameters or regularize the parameters.\n",
        "\n",
        "Here, I explain how regularization works in a logistic regression.\n",
        "\n",
        "Regularization is typically adding a penalty term to the loss function, which penalizes the coefficients of the model.\n",
        "\n",
        "There are two common types of regularization:\n",
        "\n",
        "1- L1 Regularization (Lasso Regularization):\n",
        "\n",
        "It  adds a penalty proportional to the absolute value of the coefficients leading some coefficients to be exactly zero, effectively performing feature selection.\n",
        "\n",
        "2- L2 Regularization (Ridge Regularization):\n",
        "\n",
        "It adds a penalty proportional to the square of the coefficients leading to small but non-zero coefficients."
      ],
      "metadata": {
        "id": "Gh-Ctp2dcC8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset from GITHUB\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "url = \"https://raw.githubusercontent.com/HomayounfarM/Classification/main/data/ex2data2.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.columns = ['pH', 'M3_P', 'Yield']\n",
        "df['pH_r'] = (df['pH']-np.mean(df['pH']))/np.std(df['pH'])\n",
        "df['M3_P_r'] = (df['M3_P']-np.mean(df['M3_P']))/np.std(df['M3_P'])\n",
        "\n",
        "\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "6_wMJi7c1O_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "X_scaled = np.array(df[['pH_r', 'M3_P_r']])\n",
        "YY = np.array(df['Yield'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, YY, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xkr3AYtp12DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df['pH_r']\n",
        "x2 = df['M3_P_r']\n",
        "y = df['Yield']\n",
        "\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "yp9qve8u32f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_mapped_train = poly.fit_transform(X_train)\n",
        "\n",
        "print(\"Original shape of data:\", X_train.shape)\n",
        "print(\"Shape after feature mapping:\", X_mapped_train.shape)"
      ],
      "metadata": {
        "id": "rMxQ2_eX19zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train[0]:\", X_train[0])\n",
        "print(\"mapped X_train[0]:\", mapped_X[0])"
      ],
      "metadata": {
        "id": "qLIJitG82AuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "\n",
        "    g = 1/(1+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "5yK19Ko-6uxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "\n",
        "    # number of training examples\n",
        "    m, n = X.shape\n",
        "    p = np.zeros(m)\n",
        "\n",
        "    z = X.dot(w)+b\n",
        "    p = sigmoid(z)\n",
        "    f = p >= 0.5\n",
        "\n",
        "    return p, f"
      ],
      "metadata": {
        "id": "6jbI0gIylLu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost function\n",
        "def compute_cost(X, y, w, b, lambda_= 1):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    z = X.dot(w)+b\n",
        "    f = sigmoid(z)\n",
        "    total_cost = (1/m) * (-y.T.dot(np.log(f))- (1-y).T.dot(np.log(1-f)))\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "Y_heJsnR61S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent\n",
        "def compute_gradient(X, y, w, b, lambda_=None):\n",
        "\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0.\n",
        "\n",
        "    y = y.reshape(-1,1)\n",
        "    w = w.reshape(-1,1)\n",
        "\n",
        "    f = sigmoid(X.dot(w)+b)\n",
        "    diff_fy = f-y\n",
        "    dj_dww = (1/m) * diff_fy.T.dot(X)\n",
        "    dj_dww = dj_dww.T\n",
        "    dj_dw = np.reshape(dj_dww, -1)\n",
        "    dj_db = (1/m) * (dj_db + np.sum(diff_fy))\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "_krxyN8D8WJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_):\n",
        "\n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w_in = w_in - alpha * dj_dw\n",
        "        b_in = b_in - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
        "            w_history.append(w_in)\n",
        "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
        "\n",
        "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "wuRvqnGS7UfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    cost_without_reg = compute_cost(X, y, w, b)\n",
        "\n",
        "    reg_cost = 0.\n",
        "\n",
        "    reg_cost = np.sum(w**2)\n",
        "\n",
        "    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "9g1eiY8x2DeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_reg(X, y, w, b, lambda_ = 1):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
        "\n",
        "    dj_dw = dj_dw + (lambda_/m) * w\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "I0zX-AJgT4o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display cost with w initialized to zeroes\n",
        "\n",
        "np.random.seed(1)\n",
        "initial_w = 0.01 * (np.random.rand(X_mapped_train.shape[1]) - 0.5)\n",
        "initial_b = -8\n",
        "\n",
        "lambda_ = 0.5\n",
        "\n",
        "cost = compute_cost_reg(X_mapped_train, y_train, initial_w, initial_b, lambda_)\n",
        "print(\"Regularized cost :\", cost)"
      ],
      "metadata": {
        "id": "X96-f-wFE1i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.random.seed(1)\n",
        "initial_w = 0.01 * (np.random.rand(X_mapped_train.shape[1]) - 0.5)\n",
        "initial_b = -8\n",
        "\n",
        "lambda_ = 0.05\n",
        "\n",
        "# Some gradient descent settings\n",
        "iterations = 100000\n",
        "alpha = 0.01\n",
        "\n",
        "w,b, J_history,_ = gradient_descent(X_mapped_train, y_train, initial_w, initial_b,\n",
        "                                    compute_cost_reg, compute_gradient_reg,\n",
        "                                    alpha, iterations, lambda_)\n",
        "initial_w, initial_b, w,b"
      ],
      "metadata": {
        "id": "hbIbkTs02GM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting the decision boundary\n",
        "\n",
        "Try to plot decision boundary from got in previous step."
      ],
      "metadata": {
        "id": "Yf41SEWEUsmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute accuracy on the training set\n",
        "p = predict(X_mapped_train, w, b)\n",
        "\n",
        "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
      ],
      "metadata": {
        "id": "eco4KBOR2Q5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df['pH_r']\n",
        "x2 = df['M3_P_r']\n",
        "y = df['Yield']\n",
        "\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "# Plot decision boundary\n",
        "# Generate a grid of points to plot decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1, 100),\n",
        "                     np.linspace(X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1, 100))\n",
        "\n",
        "# Create polynomial features for the grid\n",
        "grid_poly = poly.transform(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "p = predict(grid_poly, w, b)[0].reshape(xx.shape)\n",
        "\n",
        "# Create polynomial features for the grid\n",
        "grid_poly = poly.transform(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Plot decision boundary (threshold at 0.5)\n",
        "contour = plt.contour(xx, yy, p, levels=[0.5], linewidths=2, colors='black')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])"
      ],
      "metadata": {
        "id": "MKjXNM5k2Vf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs.shape"
      ],
      "metadata": {
        "id": "MUKFOhrHc8W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Skit-Learn\n",
        "\n"
      ],
      "metadata": {
        "id": "GBWpqn5ZoJz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# load dataset\n",
        "XX = np.array(df[['pH', 'M3_P']])\n",
        "YY = np.array(df['Yield'])\n",
        "\n",
        "#Feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scale=StandardScaler()\n",
        "X_scaled = scale.fit_transform(XX)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, YY, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_mapped_train = poly.fit_transform(X_train)\n",
        "X_mapped_test = poly.fit_transform(X_test)\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "# C parameter controls the regularization strength (inverse of regularization strength)\n",
        "# Higher C means less regularization (default C=1.0)\n",
        "model = LogisticRegression(penalty='l2', C=15, solver='lbfgs', max_iter=10000, random_state=42)\n",
        "\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_mapped_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_mapped_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Extract the coefficients\n",
        "coefficients = model.coef_[0]\n",
        "intercept = model.intercept_[0]\n",
        "coefficients, intercept\n",
        "\n",
        "#Plotting the decision boundary\n",
        "x1 = X_scaled[:,0]\n",
        "x2 = X_scaled[:,1]\n",
        "y = YY\n",
        "\n",
        "plt.scatter(x1[y==0], x2[y==0], s=3, c='r')\n",
        "plt.scatter(x1[y==1], x2[y==1], s=3, c='b')\n",
        "\n",
        "# Plot decision boundary\n",
        "# Generate a grid of points to plot decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1, 100),\n",
        "                     np.linspace(X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1, 100))\n",
        "\n",
        "# Create polynomial features for the grid\n",
        "grid_poly = poly.transform(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Predict probabilities for each point in the grid\n",
        "probs = model.predict_proba(grid_poly)[:, 1].reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary (threshold at 0.5)\n",
        "contour = plt.contour(xx, yy, probs, levels=[0.5], linewidths=2, colors='black')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('pH_r')\n",
        "plt.ylabel('M3_P_r')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(['Low yield', 'High yield'])\n",
        "coefficients"
      ],
      "metadata": {
        "id": "SNUet9SGoIxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot data points\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=YY, cmap='coolwarm', edgecolors='k', s=100)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# Generate a grid of points to plot decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1, 100),\n",
        "                     np.linspace(X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1, 100))\n",
        "\n",
        "\n",
        "# Create polynomial features for the grid\n",
        "grid_poly = poly.transform(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Predict probabilities for each point in the grid\n",
        "probs = model.predict_proba(grid_poly)[:, 1].reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary (threshold at 0.5)\n",
        "contour = plt.contour(xx, yy, probs, levels=[0.5], linewidths=2, colors='black')\n",
        "\n",
        "#plt.clabel(contour, inline=True, fontsize=12, fmt={0.5: 'Decision Threshold'})\n",
        "\n",
        "plt.title('Logistic Regression Decision Boundary with Polynomial Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RiZDqZnpL20Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def PolyCoefficients(x, coeffs):\n",
        "    \"\"\" Returns a polynomial for ``x`` values for the ``coeffs`` provided.\n",
        "\n",
        "    The coefficients must be in ascending order (``x**0`` to ``x**o``).\n",
        "    \"\"\"\n",
        "    o = len(coeffs)\n",
        "    print(f'# This is a polynomial of order {o}.')\n",
        "    y = 0\n",
        "    for i in range(o):\n",
        "        y += coeffs[i]*x**i\n",
        "    return y\n",
        "\n",
        "x = np.linspace(0, 9, 10)\n",
        "coeffs = [1, 2, 3, 4, 5]\n",
        "plt.plot(x, PolyCoefficients(x, coeffs))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fqr-NwZF2Ydw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data with polynomial terms\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0,\n",
        "                           random_state=42, class_sep=1.0)\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit polynomial features\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "# Fit logistic regression model\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Plot decision boundary\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', s=100)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# Generate a grid of points to plot decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
        "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
        "\n",
        "\n",
        "# Create polynomial features for the grid\n",
        "grid_poly = poly.transform(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Predict probabilities for each point in the grid\n",
        "probs = model.predict_proba(grid_poly)[:, 1].reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary (threshold at 0.5)\n",
        "contour = plt.contour(xx, yy, probs, levels=[0.5], linewidths=2, colors='black')\n",
        "\n",
        "#plt.clabel(contour, inline=True, fontsize=12, fmt={0.5: 'Decision Threshold'})\n",
        "\n",
        "plt.title('Logistic Regression Decision Boundary with Polynomial Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hU5eVpYe1ZXU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}