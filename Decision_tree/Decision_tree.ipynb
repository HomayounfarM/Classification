{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPzmu8Q92r3wvLZ8bJjE/N4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HomayounfarM/Classification/blob/main/Decision_tree/Decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree"
      ],
      "metadata": {
        "id": "jAaNsT5qcWNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision tree\n",
        "Decision tree is a **non-parametric**, **supervised**, **classification** algorithm that assigns data to discrete groups.\n",
        "\n",
        "**Non-parametric**: Decision tree does NOT make assumptions about data’s distribution or structure.\n",
        "\n",
        "**Supervised**: The class of training set MUST be provided by the users.\n",
        "\n",
        "**Classification**: Decision tree classifies data into discrete classes, instead of predicting a real number.\n",
        "\n",
        "**Note**:The workflow of regression tree is similar to the decision tree, but regression tree predicts continuous values.\n",
        "\n",
        "**Root node** and **inner nodes** represent a test/question on an attribute, while root node is the beginning of the tree (first question to be asked). Further, a **branch** represents an outcome of a test, which is used to connect the nodes. Last, the **leaf nodes** are the classification result. There is no more question in the leaf nodes, but all the data in a leaf node is classified to the same class.\n",
        "\n",
        "### Parameters (**criterion**, **max_depth**, **class_weight**)\n",
        "\n",
        "Decision tree has a lot of parameters we can tune, but we’ll just go through the frequently used ones, **criterion**, **max_depth**, and **class_weight**.\n",
        "\n",
        "**criterion**: As we talked about, root node and inner nodes represent a test on an attribute. However, how do we decide which attribute to split on first (i.e. age, gender)? We can use **Gini impurity** or **Information gain** to determine . \n",
        "\n",
        "**max_depth**: **First question**, what is tree depth? Tree depth is the number of layers of a tree.\n",
        "\n",
        "**Second question**, why would we want to limit the depth of tree? We want to avoid overfitting.\n",
        "\n",
        "**Note**: Limiting tree depth is a method of pre-pruning. Namely, we prune the tree before it even grows to avoid the issue of overfitting!\n",
        "In fact, there are other pre-pruning measures, such as min_samples_split (minimum number of samples required to split an internal node) and min_samples_leaf (minimum number of samples required to be at a leaf node). There is no standard process in terms of selecting pre-pruning measures, we have to trial-and-error to determine which parameters to use.\n",
        "\n",
        "**class_weight**: the weights of the classes will affect the splitting criterion. To put it simply, splitting criterion will pay more attention to the “important” class and try to classify all the data in that class correctly. Thus, we can assign more weight to the conceptually important classes, which might differ according to the purpose of the research.\n",
        "\n",
        "**Note**: One way to choose hyper-parameters is using grid search and K-fold cross validation\n",
        "\n",
        "**Problem Statement:** we want to predict whether a passenger will survive in the Titanic (y) given the characteristics of the passenger (x). A decision tree might look like this.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "27Eav5nEgqgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "url = 'https://github.com/HomayounfarM/Classification/blob/main/Decision_tree/photo/tree.jpg?raw=true'\n",
        "page = requests.get(url)\n",
        "Image.open(BytesIO(page.content))"
      ],
      "metadata": {
        "id": "5_A13DEGTQyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder          \n",
        "from sklearn.model_selection import train_test_split  \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3QvJhavgSXul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing data\n",
        "url = 'https://raw.githubusercontent.com/HomayounfarM/Classification/main/Decision_tree/Data/titanic.txt'\n",
        "data = pd.read_csv(url)\n",
        "data.info()"
      ],
      "metadata": {
        "id": "jwPDiGTCSnrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** axis=1 means we’re dropping a column, inplace=True means we want to make modification on the original dataset."
      ],
      "metadata": {
        "id": "7SSDj156gdTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manage null data, dropping the whole column, 'Cabln'\n",
        "print(data.isnull().sum())\n",
        "# drop embarked\n",
        "data.drop('Cabin', axis=1, inplace = True)\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "OMWXtlVVe_Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of na\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "0EhzI0Op5jWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop na \n",
        "data.dropna(inplace=True)\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "_OVwvhHW4nol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "GT7dBL6r7JGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at the first few rows of the data"
      ],
      "metadata": {
        "id": "U79FMjCI7_Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "TanH1I2B79Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find the values in Sex are **strings**, ‘female’ and ‘male’. Decision tree only takes columns of numerical values, whether continuous or not. Thus, we have to convert the string to numbers, and we use **LabelEncoder** to do the conversion."
      ],
      "metadata": {
        "id": "gutOxIns8jp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "data['Sex']=le.fit_transform(data['Sex'])\n",
        "data.head()"
      ],
      "metadata": {
        "id": "X1gPGL0L9HQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decide independent variables and target variables"
      ],
      "metadata": {
        "id": "J4J8whZLPS4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
        "y = data['Survived']"
      ],
      "metadata": {
        "id": "mHCQ3hKDPDQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/Test split:"
      ],
      "metadata": {
        "id": "Z-xQNK8YPvJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=65)"
      ],
      "metadata": {
        "id": "q2cq8kMNPwta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the hyper-parameters and train a model\n",
        "we just manually select the hyper-parameters. In specific, we choose maximum tree depth as 3. In other words, we leave all the other parameters as default, and you can press tab+shift to see the default values of DecisionTreeClassifier class."
      ],
      "metadata": {
        "id": "GWpQ7gfaQRBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(max_depth=3)\n",
        "dt_model = dt.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "l4tu5QaUQK7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the tree:\n",
        "\n",
        "To have a more concrete idea, let’s visualize it! In Python, we can simply use plot_tree to visualize the tree model. To illustrate, feature_names is the names of independent variables, and class_names is the values/classes in the target attribute."
      ],
      "metadata": {
        "id": "EC744cvuQ5Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12, 10))\n",
        "tree.plot_tree(dt_model, feature_names=list(X.columns), class_names=['Not survived', 'Survived'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SRX6QIBORETn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s our tree! Indeed it only has 3 layers since we set max_depth as 3. Further, the first question our tree asks is “Is the sex of passenger ≤ 0.5?”. It seems like a weird question, but we did encode the gender attribute in step 3. The female is represented by 0; the male is represented by 1. Thus, this question should be interpreted as “Is the passenger female?”."
      ],
      "metadata": {
        "id": "5HFR7ktqRQly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model performance on testing set:\n",
        "\n",
        "The accuracy score on testing set is 0.783. Namely, 78.3% of testing set is correctly classified, which shows the model is generally fine. However, it’s always important to examine model’s performance on individual class. Therefore, we should also check out precision and recall score (see this article for detail)."
      ],
      "metadata": {
        "id": "PIaD2QzwRUQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy score on Testing set: \", dt_model.score(x_test, y_test))"
      ],
      "metadata": {
        "id": "ntMNIjojRafy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the following cells we discuss some ways manage NaN and encoding categorical veriables.\n"
      ],
      "metadata": {
        "id": "NrAJ7GOZNrXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "# Managing NaN by removing the whole row\n",
        "df = pd.DataFrame({'class':[np.nan,'m', 'l', 'h', 'h', 'm', 'l', 'h', 'm', 'l'], 'value':[5,1, 2, 3, 4, 5, 7, 8, 9, 10]})\n",
        "print(df)\n",
        "df.dropna(inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "wW47xAVqMl-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "# Note: When we have a categorical variable with more then two categories, \n",
        "# we can use OneHotEncoder() instead of LabelEncoder(). See the following example.\n",
        "df = pd.DataFrame({'class':['m', 'l', 'h', 'h', 'm', 'l', 'h', 'm', 'l']})\n",
        "le = LabelEncoder()\n",
        "df['class'] = le.fit_transform(df['class'])\n",
        "df"
      ],
      "metadata": {
        "id": "IO8uxjpb96dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "# The extra bracket in df[['class']] reshape it from (n,) to (n,m)\n",
        "df = pd.DataFrame({'class':['m', 'l', 'h', 'h', 'm', 'l', 'h', 'm', 'l']})\n",
        "onehotencoder = OneHotEncoder()\n",
        "transformed = onehotencoder.fit_transform(df[['class']]).toarray()\n",
        "transformed\n",
        "df[['High', 'Low', 'Medium']] = transformed\n",
        "df"
      ],
      "metadata": {
        "id": "MxWps2ypA0Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bPfZSf3jOh86"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}